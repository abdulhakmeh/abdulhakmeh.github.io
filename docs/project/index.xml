<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Abdul Hakmeh</title>
    <link>https://abdulhakmeh.github.io/project/</link>
      <atom:link href="https://abdulhakmeh.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 27 Sep 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://abdulhakmeh.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://abdulhakmeh.github.io/project/</link>
    </image>
    
    <item>
      <title>Anomalies Intervals Detection</title>
      <link>https://abdulhakmeh.github.io/project/outlier_detection/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://abdulhakmeh.github.io/project/outlier_detection/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This work is carried out as a PhD. application assign ment with a duration of 10 days. The task is to identify and sort possible outlier intervals from a multivariate time series weather dataset. We use a K-nearest neighbour based method to search for anomalous data points in the data. We then create intervals between the sampled points using the sliding window technique. An approximation to Kullback-Leibler divergence (KL) is used to quantify the degree of di-vergence between two Gaussian distributions (the generated interval distribution and the distribution of the remaining data). At the end, a list of the top intervals is generated to rank the anomalies based on their scores. The results show that the algorithm is able to detect numerous outlier intervals. However, due to the lack of labeled data, we could not numerically evaluate the performance of the algorithm.&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;To evaluate the methodology, we conduct experiments
with real-world multivariate time series data recorded from
ocean observing buoys provided by the National Data Buoy Center (NDBC). The dataset covers six months of hourly
data, beginning in June 2012 and ending in November of the
same year. This period corresponds to the Atlantic hurricane
season, which was particularly active this year with 19
tropical cyclones. 10 of them became hurricanes (winds
over 64 km/h). This information can help to interpret
and evaluate the results by matching the extracted interval
with the already known time window of hurricanes since
the dataset does not provide ground-truth data. Due to the
limited time, we skip the matching.
The variables provided are measurements of significant
wave height Hs, wind speed W, and sea level pressure P.
The data were collected at a site near the Bahamas in the
Atlantic Ocean 1 (23.866° N, 68.481° W).&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The above figure illustrates the results of applying the KL al-
gorithm based on generated intervals containing pointwise
outliers. The figure shows three time series curves represent-
ing the features of the data set. The red color highlights the
area where the outlier intervals are detected. The numbers
at the top indicate the order of the intervals based on their
intensity. Number 1 has the highest intensity. The results
show that the algorithm is able to detect numerous outlier
intervals, most of which could be identified by eye. Others,
like interval number 4 are hard to identify directly. However,
we could not confirm the results due to the lack of labeled
data.&lt;/p&gt;
&lt;p&gt;For the detailed information about the utilized methods, please refer to my GitHub repository to view the source code or to the attached report file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Investigating the effects of non-uniform input data windowing on electrical load disaggregation performance</title>
      <link>https://abdulhakmeh.github.io/project/non-linear-windowing/</link>
      <pubDate>Tue, 08 Mar 2022 17:13:22 +0200</pubDate>
      <guid>https://abdulhakmeh.github.io/project/non-linear-windowing/</guid>
      <description>&lt;p&gt;Non-intrusive Load Monitoring (NILM) is an application of smart meters to decompose the aggregated electrical load consumption into individual appliance profiles. State-of-the-art NILM methods based on neural networks showed remarkable disaggregation accuracy results. Internally, the network input data are sampled based on a windowing technique with a fixed window length limiting the number of input data, affecting the disaggregation performance. In addition, the use of different window length configurations per device increases the complexity of the model.&lt;/p&gt;
&lt;p&gt;In this project, a windowing method is proposed using nonlinear algebraic functions to capture the input data points. The goal is to investigate the effect of expanding the time interval of the input window on load disaggregation without influencing the model complexity. Non-equidistant temporal sampling technique splits the traditional window into two sequences according to a fraction: a high-resolution sequence and
an non-equally spaced sequence generated by means of non-linear algebraic functions, allowing the input window to contain more historical information without having to adjust the window length parameter. For evaluating the proposed technique, different variants of non-equidistant temporal sampling are suggested, the inverse and the linear downsampling function. The inverse  generates an entirely non-equally spaced window, and the linear function creates a fraction of the window with equally time-spaced samples. Both functions cover the same effective window length as the proposed method.&lt;/p&gt;
&lt;p&gt;The results confirm a positive impact of aggregating the historical information on load disaggregation. The promising potential of the proposed technique shown during the evaluation can make a step forward in the NILM field, as collecting historical input data according to a non-equidistant or linear manner ensure an improvement in the performance without affecting the model complexity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimization fuel-consuming in platooning of vehicles by using sophisticated meta-heuristics</title>
      <link>https://abdulhakmeh.github.io/project/simulated-annealing-for-platooning/</link>
      <pubDate>Thu, 09 Apr 2020 13:24:16 +0200</pubDate>
      <guid>https://abdulhakmeh.github.io/project/simulated-annealing-for-platooning/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Platooning of vehicles is a novel approach to reduce fuel consumption in long-distance transportation services by establishing a string of vehicles driving on the road close behind each other to reduce the aerodynamic drag. The motivation relies on the fact that the fuel costs are held accountable for the third of the total operational costs of those services, besides being the Heavy-duty vehicles one of the carbon emissions sources.&lt;/p&gt;
&lt;p&gt;In this study, we examine the important but complex problem of assigning optimal routes to a set of vehicles, where many platooning potentials can be offered. We use the Simulated Annealing metaheuristic approach to solve the centralized routing problem with realistic assumptions on a real-world map. A robust design method is used to tune the model parameters. Unlike the previous contributions, a restriction on the platoon size is proposed to avoid traffic congestion. For a fair evaluation, a mathematical optimization solver using Gurobi is implemented based on a mathematical formulation of the problem. An additional sensitivity analysis is made to study the impact of different model parameters on the output.&lt;/p&gt;
&lt;p&gt;In practice applying simulated annealing achieved approximately 5% fuel saving with an instance that has 500 trucks. For evaluating the algorithm on a real-live map the algorithm scored near 4.5% of fuel saving. The findings confirmed a satisfactory performance of the approach over the exact solver even when the restriction of the platoon size is applied, which can have a positive influence at the forefront of the platooning field, as solving the problem and enhance the solution in a reasonable time can ensure a globally optimized fuel cost-saving for the participants  besides bringing environmental benefits.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the Impact of Data Sampling Rates on Event Detection Accuracy in Load Signatures using a Shapelet based Approach</title>
      <link>https://abdulhakmeh.github.io/project/shapelet-based-event-detection/</link>
      <pubDate>Thu, 08 Aug 2019 13:39:15 +0200</pubDate>
      <guid>https://abdulhakmeh.github.io/project/shapelet-based-event-detection/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Non-intrusive load monitoring (NILM) is one of the practical applications of the smart electricity grid. NILM provides a comprehensive road-map to detect and determine operation circumstances of an individual appliance from an aggregate load. It can also provide the energy management system with information about appliance-level power consumption, analytical statistics, and detection of energy-hungry appliances to ensure the efficient use of the resources. The event detection is one of the NILM phases that is responsible for detecting state-changes of the appliances.&lt;/p&gt;
&lt;p&gt;In this project, a study is made to measure the impact of data sampling rate on the detection accuracy obtained through an event detection algorithm. Therefore, a machine learning based shapelet approach for electrical appliance use detection is implemented. The fast shapelet approach detects the occurrence of an event that is caused by switching on/off of an electrical appliance. A general architecture is introduced to evaluate the performance through feeding the algorithm with re-sampled data and comparing the obtained accuracy with the sampling rate. Different metrics are used to ensure a comprehensive evaluation; F1-score, confusion matrix, and classification accuracy.&lt;/p&gt;
&lt;h2 id=&#34;data-set&#34;&gt;Data set&lt;/h2&gt;
&lt;p&gt;To evaluate our algorithm the BLUED dataset is used. The BLUED includes one week of the current and the voltage measurements for a family house in Pennsylvania, Pittsburgh. The data collection were taken in October 2011 from nearly 50 electrical devices in the house. The dataset presents the changes in the operating status of each appliance (turn  on/off). The measurements were taken by using two separate systems; one is at the main distribution panel to capture the current and voltage, and the other system is used to register the time stamps for each event. The reason behind choosing the BLUED is that this dataset in its original form provides raw data of the current and the voltage.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The results of the experiment have shown that it is possible to detect appliances events with downsampled data using a shapelet based approach. Furthermore, it would have been as well possible that the downsampling improves the accuracy, but the evaluation of the event detection on noisy downsampled data have statistically verified that the lowering of the sampling rate is directly proportional to the accuracy of the detection in a linear form.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
